# ROJUDGER - GuÃ­a de Colas con Redis

## ğŸ¯ Â¿QuÃ© Lograste?

Has implementado un sistema de **colas asÃ­ncronas con Redis** en ROJUDGER que permite:

âœ… **Escalabilidad**: MÃºltiples workers procesando cÃ³digo en paralelo  
âœ… **AsincronÃ­a**: API responde instantÃ¡neamente sin esperar ejecuciÃ³n  
âœ… **Confiabilidad**: Si un worker falla, otro puede retomar  
âœ… **PriorizaciÃ³n**: Colas de alta/normal/baja prioridad  
âœ… **Monitoreo**: EstadÃ­sticas en tiempo real de la cola  

---

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Cliente â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚ POST /submissions
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  API Server  â”‚ â† Encola en Redis
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Redis Queue  â”‚ â† Lista de trabajos
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Worker(s)   â”‚ â† Procesan cÃ³digo
â”‚  (1...N)     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL  â”‚ â† Guarda resultados
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ CÃ³mo Usar

### OpciÃ³n 1: Modo DIRECTO (sin cola, sÃ­ncro)

El API ejecuta el cÃ³digo directamente y responde cuando termina.

```bash
# Ejecutar API en modo directo
go run cmd/api/main.go

# O con variable de entorno
USE_QUEUE=false go run cmd/api/main.go
```

**CaracterÃ­sticas:**
- âš¡ Respuesta inmediata con resultados
- ğŸ”’ Limitado a 5 ejecuciones concurrentes
- ğŸ¯ Ãštil para desarrollo/testing

### OpciÃ³n 2: Modo QUEUE (con cola, async)

El API encola el trabajo y los workers lo procesan.

```bash
# Terminal 1: API en modo queue
USE_QUEUE=true go run cmd/api/main.go

# Terminal 2+: Workers (puedes abrir varios)
go run cmd/worker/main.go
go run cmd/worker/main.go  # Worker adicional
go run cmd/worker/main.go  # Otro mÃ¡s...
```

**CaracterÃ­sticas:**
- ğŸš€ API responde instantÃ¡neamente
- ğŸ“ˆ Escala horizontalmente (mÃ¡s workers = mÃ¡s capacidad)
- ğŸ”„ Workers procesan en background
- ğŸ’ª Robusto ante fallos

---

## ğŸ“Š Endpoints del API

### POST /api/v1/submissions

**Modo Directo (USE_QUEUE=false):**
```bash
curl -X POST http://localhost:8080/api/v1/submissions \
  -H "Content-Type: application/json" \
  -d '{
    "language_id": 71,
    "source_code": "print(123)"
  }'

# Respuesta: Resultado completo (espera ~2 segundos)
```

**Modo Queue (USE_QUEUE=true):**
```bash
# Enviar y recibir ID inmediatamente
curl -X POST http://localhost:8080/api/v1/submissions \
  -H "Content-Type: application/json" \
  -d '{
    "language_id": 71,
    "source_code": "print(123)"
  }'

# Respuesta inmediata:
{
  "id": "abc-123",
  "status": "queued",
  ...
}

# Consultar resultado despuÃ©s
curl http://localhost:8080/api/v1/submissions/abc-123
```

**Modo Queue con wait=true (hÃ­brido):**
```bash
curl -X POST "http://localhost:8080/api/v1/submissions?wait=true" \
  -H "Content-Type: application/json" \
  -d '{"language_id": 71, "source_code": "print(123)"}'

# Se encola pero espera hasta 30s por el resultado
```

### GET /api/v1/queue/stats (solo modo queue)

```bash
curl http://localhost:8080/api/v1/queue/stats
```

**Respuesta:**
```json
{
  "queue_high": 0,
  "queue_default": 5,
  "queue_low": 2,
  "processing": 3,
  "total_pending": 7,
  "total_enqueued": "1250",
  "total_completed": "1200",
  "total_failed": "5"
}
```

---

## âš™ï¸ ConfiguraciÃ³n

### Variables de Entorno

AÃ±ade a tu `.env`:

```bash
# Modo de operaciÃ³n
USE_QUEUE=true          # true=queue, false=direct

# Redis
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=
REDIS_DB=0

# Workers
EXECUTOR_MAX_CONCURRENT=5  # Workers por proceso
```

---

## ğŸ”„ Flujo Completo de Ejemplo

```bash
# 1. Iniciar servicios base
docker-compose up -d postgres redis

# 2. Iniciar API en modo queue
USE_QUEUE=true go run cmd/api/main.go &

# 3. Iniciar 3 workers
for i in {1..3}; do
  go run cmd/worker/main.go &
done

# 4. Enviar cÃ³digo
SUBMISSION_ID=$(curl -s -X POST http://localhost:8080/api/v1/submissions \
  -H "Content-Type: application/json" \
  -d '{
    "language_id": 71,
    "source_code": "import time\ntime.sleep(2)\nprint(\"Done!\")"
  }' | jq -r '.id')

echo "Submission ID: $SUBMISSION_ID"

# 5. Ver stats
curl -s http://localhost:8080/api/v1/queue/stats | jq

# 6. Esperar y ver resultado
sleep 5
curl -s http://localhost:8080/api/v1/submissions/$SUBMISSION_ID | jq
```

---

## ğŸ“ˆ Escalamiento

### Escalar Verticalmente (MÃ¡s workers por mÃ¡quina)

Edita `.env`:
```bash
EXECUTOR_MAX_CONCURRENT=10  # Cada worker process ejecuta 10 concurrentes
```

### Escalar Horizontalmente (MÃ¡s mÃ¡quinas)

```bash
# Servidor 1: API + Redis + PostgreSQL
USE_QUEUE=true go run cmd/api/main.go

# Servidor 2-N: Solo workers
DB_HOST=servidor1 REDIS_HOST=servidor1 go run cmd/worker/main.go
```

---

## ğŸ” Monitoreo

### Ver Cola en Redis

```bash
# Conectar a Redis
docker exec -it rojudger-redis redis-cli

# Ver tamaÃ±o de colas
LLEN rojudger:queue:high
LLEN rojudger:queue:default
LLEN rojudger:queue:low

# Ver trabajos en procesamiento
SMEMBERS rojudger:processing

# Ver estadÃ­sticas
HGETALL rojudger:stats
```

### Ver Logs

```bash
# API
tail -f /tmp/rojudger-api.log

# Workers
# Los workers loguean en stdout, redirige a archivo si quieres:
go run cmd/worker/main.go > /tmp/worker1.log 2>&1 &
tail -f /tmp/worker1.log
```

---

## ğŸ› ï¸ Casos de Uso

### Desarrollo Local
```bash
# Modo directo, mÃ¡s simple
USE_QUEUE=false go run cmd/api/main.go
```

### ProducciÃ³n PequeÃ±a
```bash
# 1 API + 2 workers
USE_QUEUE=true go run cmd/api/main.go &
go run cmd/worker/main.go &
go run cmd/worker/main.go &
```

### ProducciÃ³n Grande
```bash
# 3 APIs (load balanced) + 10 workers distribuidos
# Servidor 1:
USE_QUEUE=true ./rojudger-api &

# Servidores 2-5 (workers):
for i in {1..10}; do
  ./rojudger-worker &
done
```

---

## ğŸ§ª Testing

```bash
# Test de carga
for i in {1..100}; do
  curl -X POST http://localhost:8080/api/v1/submissions \
    -H "Content-Type: application/json" \
    -d '{"language_id": 71, "source_code": "print('$i')"}' &
done

# Ver estadÃ­sticas
watch -n 1 'curl -s http://localhost:8080/api/v1/queue/stats | jq'
```

---

## â“ FAQ

### Â¿CuÃ¡ndo usar modo directo vs queue?

- **Directo**: Desarrollo, testing, trÃ¡fico bajo (<10 req/min)
- **Queue**: ProducciÃ³n, trÃ¡fico medio/alto (>10 req/min)

### Â¿CuÃ¡ntos workers necesito?

Regla general: `workers = peticiones_por_segundo * tiempo_promedio_ejecuciÃ³n`

Ejemplo: 10 req/s, 5 seg promedio â†’ 50 workers

### Â¿QuÃ© pasa si un worker se cae?

El trabajo queda en Redis. Cuando vuelva a subir (o otro worker), lo retoma.

### Â¿Puedo mezclar ambos modos?

SÃ­! Puedes tener instancias del API en modo directo Y en modo queue simultÃ¡neamente.

---

## ğŸ¯ PrÃ³ximos Pasos

1. **Dashboard**: Crear interfaz web para ver estadÃ­sticas
2. **Webhooks**: Notificar cuando un trabajo termine
3. **Prioridades**: Permitir al usuario elegir prioridad
4. **Reintentos**: Reintentar automÃ¡ticamente trabajos fallidos
5. **TTL**: Limpiar trabajos viejos automÃ¡ticamente

---

**Â¡Felicitaciones!** ğŸ‰ Ahora tienes un sistema de ejecuciÃ³n de cÃ³digo con colas que puede escalar horizontalmente.
